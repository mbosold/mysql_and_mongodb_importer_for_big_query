{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google BigQuery Importer for MySQL and MongoDB Data (Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import nessary libaries and load config file\n",
    "from sqlalchemy import create_engine\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from urllib import request, parse\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import datalab.storage as gcs\n",
    "import import_config as config\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to connect to MySQL database, creates engine, create Panda dataframes for all tables, transform data and creates csv files in Google Cloud Storage\n",
    "def mysql_to_gcs():\n",
    "    # Iterate through all existing mysql databases of config file\n",
    "    for idx, mysql_database in enumerate(config.mysql['dbs']):\n",
    "        # Connect to mysql db and load table and field whitelist and blacklist configuration\n",
    "        mysql_client_url = 'mysql+mysqldb://'+mysql_database['user']+':'+mysql_database['pw']+'@'+mysql_database['host']+\"/\"+mysql_database['name']\n",
    "        mysql_engine = create_engine(mysql_client_url)\n",
    "        mysql_table_names = mysql_engine.table_names()\n",
    "        use_field_whitelist = mysql_database['use_field_whitelist']\n",
    "        field_whitelist = mysql_database['field_whitelist']\n",
    "        use_field_blacklist = mysql_database['use_field_blacklist']\n",
    "        field_blacklist = mysql_database['field_blacklist']        \n",
    "        # Iterate through all tables of current mysql database\n",
    "        for mysql_table in mysql_table_names:\n",
    "            # Check if whitelisting is active and if current table is listed in the whitelist\n",
    "            if (mysql_database['use_table_whitelist'] == False) or (mysql_database['use_table_whitelist']== True and mysql_table in mysql_database['table_whitelist']):\n",
    "                # Select table data of mysql db\n",
    "                query='SELECT * FROM '+mysql_table \n",
    "                # Google BigQuery doesnt allow \"-\" in table names, so we replace it with underscores\n",
    "                db_filename = mysql_database['name'].replace(\"-\", \"_\")\n",
    "                # Create file names and paths for CSV and JSON export files\n",
    "                file_name_csv = 'mysql_'+db_filename+'_'+mysql_table+'.csv'\n",
    "                file_name_json = 'mysql_'+db_filename+'_'+mysql_table+'.json'\n",
    "                file_name_csv_path=os.path.join(config.general['csv_cache_folder'],file_name_csv)\n",
    "                file_name_json_path=os.path.join(config.general['csv_cache_folder'],file_name_json)\n",
    "                # Create dataframe for table\n",
    "                table_data = pd.read_sql_query(query, mysql_engine)\n",
    "                # Move to next table if MySQL tables is empty\n",
    "                if len(table_data.index) == 0:\n",
    "                    continue\n",
    "                # Transform dataframe with given function\n",
    "                table_data = transform_dataframe(table_data,mysql_table,use_field_whitelist,field_whitelist,use_field_blacklist,field_blacklist)\n",
    "                # Create BigQuery data schema for transformed dataframe with given function\n",
    "                table_data_schema = generate_bq_schema(table_data, default_type='STRING')\n",
    "                # Local export of csv file\n",
    "                table_data.to_csv(file_name_csv_path, sep='|', quoting=csv.QUOTE_NONNUMERIC, index=False, encoding='utf8')\n",
    "                # Local export to json file\n",
    "                with open(file_name_json_path, 'w') as fp:\n",
    "                    json.dump(table_data_schema, fp)\n",
    "                # Upload local CSV File to Google Cloud Storage\n",
    "                upload_to_google_cloud_storage(file_name_csv,file_name_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to connect to MongoDB database, creates engine, create Panda dataframes for all tables, transform data and creates csv files in Google Cloud Storage\n",
    "def mongo_to_gcs():\n",
    "    # Iterate through all existing MongoDB databases of config file\n",
    "    for idx, mongo_database in enumerate(config.mongo['dbs']):\n",
    "        # Connect to MongoDB and load collection and field whitelist and blacklist configuration\n",
    "        mongo_client_url = 'mongodb+srv://'+mongo_database['user']+':'+mongo_database['pw']+'@'+mongo_database['host']  \n",
    "        mongo_client = pymongo.MongoClient(mongo_client_url, 27017)   \n",
    "        mongo_db = mongo_client[mongo_database['name']]\n",
    "        mongo_collection_names = mongo_db.list_collection_names()\n",
    "        use_field_whitelist = mongo_database['use_field_whitelist']\n",
    "        field_whitelist = mongo_database['field_whitelist'] \n",
    "        use_field_blacklist = mongo_database['use_field_blacklist']\n",
    "        field_blacklist = mongo_database['field_blacklist']           \n",
    "        # Iterate through all collections of MongoDB database\n",
    "        for mongo_collection in mongo_collection_names:\n",
    "            # Check if whitelisting is active and if current collection is listed in the whitelist\n",
    "            if (mongo_database['use_collection_whitelist'] == False) or (mongo_database['use_collection_whitelist']== True and mongo_collection in mongo_database['collection_whitelist']):\n",
    "                # Select collection of mongo db\n",
    "                mongo_collection_client = mongo_db[mongo_collection]\n",
    "                cursor = mongo_collection_client.find({})\n",
    "                # Google BigQuery doesnt allow \"-\" in table names, so we replace it with underscores\n",
    "                db_filename = mongo_database['name'].replace(\"-\", \"_\")\n",
    "                # Create file names and paths for CSV and JSON export files\n",
    "                file_name_csv = 'mongo_'+db_filename+'_'+mongo_collection+'.csv'\n",
    "                file_name_csv_path=os.path.join(config.general['csv_cache_folder'],file_name_csv)\n",
    "                file_name_json = 'mongo_'+db_filename+'_'+mongo_collection+'.json'\n",
    "                file_name_json_path=os.path.join(config.general['csv_cache_folder'],file_name_json)\n",
    "                # Create dataframe for collection\n",
    "                table_data = pd.DataFrame(list(cursor))\n",
    "                # Transform dataframe with given function\n",
    "                table_data = transform_dataframe(table_data,mongo_collection,use_field_whitelist,field_whitelist,use_field_blacklist,field_blacklist)\n",
    "                # Create BigQuery data schema for transformed dataframe with given function\n",
    "                table_data_schema = generate_bq_schema(table_data, default_type='STRING')\n",
    "                # Local export of csv file\n",
    "                table_data.to_csv(file_name_csv_path, sep='|', quoting=csv.QUOTE_NONNUMERIC, float_format='%.2f', index=False, encoding='utf8')\n",
    "                # Local export to json file\n",
    "                with open(file_name_json_path, 'w') as fp:\n",
    "                    json.dump(table_data_schema, fp)\n",
    "                # Upload local CSV File to Google Cloud Storage\n",
    "                upload_to_google_cloud_storage(file_name_csv,file_name_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to read all objects of a given Google Cloud Storage Bucket and create Google BigQuery tables in a dataset for each\n",
    "def gcs_to_gbq():\n",
    "    # Connect to Google Cloud Storage and set bucket \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(config.gcp['bucket_name'])\n",
    "    blobs = bucket.list_blobs()\n",
    "    client = bigquery.Client()\n",
    "    # Connect to Google BigQuery and define BigQuery options\n",
    "    dataset_ref = client.dataset(config.gcp['dataset_id'])\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.allow_quoted_newlines = True\n",
    "    job_config.allow_jagged_rows = True    \n",
    "    job_config.autodetect = False\n",
    "    job_config.skip_leading_rows = 1\n",
    "    job_config.field_delimiter = \"|\"\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    # Iterate though all files of defined Google Cloud Storage bucket\n",
    "    for blob in blobs:\n",
    "        # Set dynamic URL for current Cloud Storage file and BigQuery schema file\n",
    "        uri = 'gs://'+config.gcp['bucket_name']+'/'+blob.name\n",
    "        file_name_json = blob.name.replace(\".csv\",\".json\")\n",
    "        file_name_json_path=os.path.join(config.general['csv_cache_folder'],file_name_json)\n",
    "        # Load JSON File for schema and set schema fields for BigQuery\n",
    "        input_json = open(file_name_json_path)\n",
    "        input_json_config = json.load(input_json)\n",
    "        job_config.schema = [\n",
    "             bigquery.SchemaField(item[\"name\"], item[\"type\"]) for item in input_json_config[\"fields\"]\n",
    "        ]     \n",
    "        # Set dynamic table name for Google BigQuery\n",
    "        table_name = blob.name.replace(\".csv\", \"\")\n",
    "        # Create new big query table / replace existing\n",
    "        load_job = client.load_table_from_uri(uri, dataset_ref.table(table_name), job_config=job_config) \n",
    "        assert load_job.job_type == 'load'\n",
    "        load_job.result()  # Waits for table load to complete.\n",
    "        assert load_job.state == 'DONE'\n",
    "    # Send slack message when load job done    \n",
    "    send_message_to_slack(':bigquery: Backend Data Sync: Update DWH Data OK :thumbs-up-green:', config.slack_hooks['slack_channel_1']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create dynamic Google BigQuery data schema from pandas dataframe types\n",
    "# Create mapping between Pandas dataframe types and Google BigQuery field types\n",
    "def generate_bq_schema(dataframe, default_type='STRING'):\n",
    "    type_mapping = {\n",
    "        'i': 'INTEGER',\n",
    "        'b': 'BOOLEAN',\n",
    "        'f': 'FLOAT',\n",
    "        'O': 'STRING',\n",
    "        'S': 'STRING',\n",
    "        'U': 'STRING',\n",
    "        'M': 'TIMESTAMP'\n",
    "    }\n",
    "    # Use the mapping and create BigQuery schema object for dataframe\n",
    "    fields = []\n",
    "    for column_name, dtype in dataframe.dtypes.iteritems():\n",
    "        fields.append({'name': column_name,\n",
    "                       'type': type_mapping.get(dtype.kind, default_type)})\n",
    "    return {'fields': fields}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to apply column whitelisting/blacklisting and prepare fields for BigQuery upload\n",
    "def transform_dataframe(dataframe,table_name,use_field_whitelist,field_whitelist,use_field_blacklist,field_blacklist):\n",
    "    # Iterate through all existing columns of current dataframe\n",
    "    for column in dataframe:\n",
    "        table_column_name = table_name+\".\"+column\n",
    "        # use this function to export all existing columns of whitelisted database tables with the next line\n",
    "        # export_table_fields(table_column_name)  \n",
    "        # When whitelist is activated for the database, drop all fields, which are not in whitelist\n",
    "        if (use_field_whitelist == True and table_column_name not in field_whitelist):        \n",
    "            dataframe.drop(column, axis=1, errors='ignore', inplace=True)\n",
    "            continue\n",
    "        # When blacklist is activated for the database, drop all fields, which are in blacklist\n",
    "        if (use_field_blacklist == True and table_column_name in field_blacklist):        \n",
    "            dataframe.drop(column, axis=1, errors='ignore', inplace=True)\n",
    "            continue      \n",
    "        # Find all fields with '\\n' and '\\r' and replace them with white spaces\n",
    "        if dataframe[column].dtype != np.number:\n",
    "            dataframe[column].replace('\\n',' ', inplace=True, regex=True)\n",
    "            dataframe[column].replace('\\r',' ', inplace=True, regex=True)\n",
    "    # Find all fields, where the fieldname starts with \"_\" and remove the first character from the field name\n",
    "    dataframe.rename(columns=lambda x: x[1:] if (x[0]=='_') else x, inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to send success or fail messages to configured slack channel for monitoring reasons\n",
    "def send_message_to_slack(alert_message, slack_hook):\n",
    "    post = {\"text\": \"{0}\".format(alert_message)}\n",
    "    json_data = json.dumps(post)\n",
    "    req = request.Request(slack_hook, data=json_data.encode('ascii'), headers={'Content-Type': 'application/json'}) \n",
    "    resp = request.urlopen(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to write error to a local log file\n",
    "def write_error_to_log(error_message):\n",
    "    i = datetime.now()\n",
    "    logfile_name = 'log-'+i.strftime('%Y-%m-%d')+'.txt'\n",
    "    logfile_path = os.path.join(config.general['log_file_folder'],logfile_name)\n",
    "    logfile = open(logfile_path,'a+')   \n",
    "    logfile.write(i.strftime('%Y-%m-%d %H:%M:%S')+' '+config.general['python_script_name']+': '+error_message+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to upload files to Google CLoud Storage bucket from config\n",
    "def upload_to_google_cloud_storage(file_name_csv,file_name_csv_path):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(config.gcp['bucket_name'])\n",
    "    blob = bucket.blob(file_name_csv)\n",
    "    blob.upload_from_filename(file_name_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Help function to write all fields of whitelisted tables to a local log file for copy & pasting to fields whitelist of config file\n",
    "def export_table_fields(field):\n",
    "    logfile_name = 'fields.txt'\n",
    "    logfile_path = os.path.join(config.general['csv_cache_folder'],logfile_name)\n",
    "    logfile = open(logfile_path,'a+')   \n",
    "    logfile.write(field+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execution of complete job\n",
    "try:\n",
    "    # execute mysql_to_gcs function for all mysql databases\n",
    "    mysql_to_gcs()\n",
    "    # execute mongo_to_gcs function for all mongo databases\n",
    "    mongo_to_gcs()\n",
    "    # execute gcs_to_gbq function for google cloud storage buckets\n",
    "    gcs_to_gbq()\n",
    "except Exception as em:\n",
    "    # Send message to slack for failed job and write error log\n",
    "    send_message_to_slack(':bigquery: Backend Data Sync: Update DWH Data Failed :no_entry:', config.slack_hooks['slack_channel_1'])\n",
    "    write_error_to_log(str(em))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
